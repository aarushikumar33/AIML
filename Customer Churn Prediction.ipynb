{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cfc82e2",
   "metadata": {},
   "source": [
    "# `Data Description`\n",
    "\n",
    "Dataset consists customer information for a customer churn prediction problem. It includes the following columns:\n",
    "\n",
    "**CustomerID**: Unique identifier for each customer.\n",
    "\n",
    "\n",
    "**Name**: Name of the customer.\n",
    "\n",
    "\n",
    "**Age: Age of the customer.**\n",
    "\n",
    "\n",
    "**Gender**: Gender of the customer (Male or Female).\n",
    "\n",
    "\n",
    "**Location**: Location where the customer is based, with options including Houston, Los Angeles, Miami, Chicago, and New York.\n",
    "\n",
    "\n",
    "**Subscription_Length_Months**: The number of months the customer has been subscribed.\n",
    "\n",
    "\n",
    "**Monthly_Bill**: Monthly bill amount for the customer.\n",
    "\n",
    "\n",
    "**Total_Usage_GB**: Total usage in gigabytes.\n",
    "\n",
    "\n",
    "**Churn**: A binary indicator (1 or 0) representing whether the customer has churned (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11fb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "853c8571",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/Users/aarushikumar/Downloads/Customer-Churn-Prediction-main'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/aarushikumar/Downloads/Customer-Churn-Prediction-main\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    477\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(io, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, engine\u001b[38;5;241m=\u001b[39mengine)\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py:1496\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1496\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[1;32m   1497\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m   1498\u001b[0m     )\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1501\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1502\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1503\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py:1371\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1369\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m   1372\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1374\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1375\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/Users/aarushikumar/Downloads/Customer-Churn-Prediction-main'"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(r'/Users/aarushikumar/Downloads/Customer-Churn-Prediction-main')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaafb09",
   "metadata": {},
   "source": [
    "# `Exploratory Data Analysis (EDA)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12046ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b2b8a",
   "metadata": {},
   "source": [
    "**All variables has correct data type.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13edb16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()/len(df)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7d2a0",
   "metadata": {},
   "source": [
    "**There is no null value in any of variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b833ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e183a",
   "metadata": {},
   "source": [
    "**There is no duplicate record in datset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ad6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical variables\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09996b54",
   "metadata": {},
   "source": [
    "### `Age` \n",
    "\n",
    "* **The average age of the customers is approximately 44 years.**\n",
    "\n",
    "\n",
    "* **The ages of the people varies widely. The youngest person is 18 years old and the oldest person is 70 years old.**\n",
    "\n",
    "\n",
    "* **The majority of customers fall within the range of 31 to 57 years.**\n",
    "\n",
    "\n",
    "* **The spread of ages is represented by a standard deviation of about 15 years.**\n",
    "\n",
    "\n",
    "### `Subscription_Length_Months` \n",
    "\n",
    "* **On average, customers have a subscription length of around 12.5 months.**\n",
    "\n",
    "\n",
    "* **Subscription lengths vary between 1 and 24 months.**\n",
    "\n",
    "\n",
    "* **The majority of customers have subscription lengths ranging from 6 to 19 months.**\n",
    "\n",
    "\n",
    "* **The standard deviation is approximately 6.9 months, indicating some variation in subscription lengths.**\n",
    "\n",
    "\n",
    "### `Monthly_Bill`\n",
    "\n",
    "* **Customers pay an average monthly bill of roughly $65.**\n",
    "\n",
    "\n",
    "* **Monthly bills range from 30 Dollar (minimum) to 100 Dollar (maximum).**\n",
    "\n",
    "\n",
    "* **The majority of customers pay between approximately 47.50 Dollar to 82.64 Dollar per month.**\n",
    "\n",
    "\n",
    "* **Monthly bill amounts have a standard deviation of about $20.23, showing some variability.**\n",
    "\n",
    "\n",
    "### `Total_Usage_GB`\n",
    "\n",
    "* **The average total usage of customers is about 274.4 GB.**\n",
    "\n",
    "\n",
    "* **Total usage varies between 50 GB (minimum) and 500 GB (maximum).**\n",
    "\n",
    "\n",
    "* **Most customers have total usage between 161 GB and 387 GB.**\n",
    "\n",
    "\n",
    "* **Total usage has a standard deviation of approximately 130.46 GB, indicating a significant range.**\n",
    "\n",
    "### `Churn`\n",
    "\n",
    "* **About half of the customers have churned (Churn value = 1), while the other half haven't (Churn value = 0).**\n",
    "\n",
    "\n",
    "* **This is represented by a mean churn rate of approximately 0.5.**\n",
    "\n",
    "\n",
    "* **Churn values are binary (0 or 1), indicating whether a customer has churned or not.**\n",
    "\n",
    "\n",
    "* **Churn rate is evenly distributed due to a mean close to 0.5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of categoical variables\n",
    "\n",
    "df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68616266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190386dc",
   "metadata": {},
   "source": [
    "### `Gender`\n",
    "\n",
    "* **The dataset includes information about two unique genders.**\n",
    "\n",
    "\n",
    "* **The most common gender is \"Female,\" which appears 50,216 times.**\n",
    "\n",
    "\n",
    "* **The frequency of females in the dataset is greater than that Male.**\n",
    "\n",
    "\n",
    "### `Location`\n",
    "\n",
    "* **The dataset contains information about five unique locations[Houston, Los Angeles, Miami, Chicago, New York].**\n",
    "\n",
    "\n",
    "* **The most frequent location is \"Houston,\" occurring 20,157 times.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804da930",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Checking correlation between numerical variables\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df\u001b[38;5;241m.\u001b[39mcorr()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Checking correlation between numerical variables\n",
    "\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29e0eb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m5\u001b[39m)) \u001b[38;5;66;03m#(width,height)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(corr_matrix, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m, square\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(10,5)) #(width,height)\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6531a4",
   "metadata": {},
   "source": [
    "### `Age and Subscription Length` \n",
    "\n",
    "* **Age and subscription length have a very low positive correlation of approximately 0.0034.**\n",
    "\n",
    "\n",
    "* **This indicates that there is almost no linear relationship between a customer's age and the length of their subscription. It means customer's age does not have a strong impact on how long they stay subscribed.**\n",
    "\n",
    "\n",
    "### `Age and Monthly Bill`\n",
    "\n",
    "* **Age and monthly bill amount have an extremely low positive correlation of about 0.0011.**\n",
    "\n",
    "\n",
    "* **This suggests that there is almost no linear relationship between a customer's age and the amount they are billed monthly. It means customer's age does not have a strong impact on the amount they are billed monthly.**\n",
    "\n",
    "\n",
    "### `Age and Total Usage`\n",
    "\n",
    "* **Age and total usage in GB have a very low positive correlation of around 0.0019.**\n",
    "\n",
    "\n",
    "* **This implies that there is almost no linear relationship between a customer's age and the amount of data they use. it means  customer's age does not have a strong impact on how much data they use.**\n",
    "\n",
    "\n",
    "### `Age and Churn`\n",
    "\n",
    "* **Age and churn have a very low positive correlation of about 0.0016.**\n",
    "\n",
    "\n",
    "* **This means that there is almost no linear relationship between a customer's age and their likelihood to churn means customer's age is not a good predictor of whether they will churn.**\n",
    "\n",
    "\n",
    "### `Subscription Length and Monthly Bill`\n",
    "\n",
    "* **Subscription length and monthly bill amount have a very low negative correlation of approximately -0.0053.**\n",
    "\n",
    "\n",
    "* **There is a weak relationship between how long a customer has been subscribed and how much they pay each month.**\n",
    "\n",
    "\n",
    "### `Subscription Length and Total Usage`\n",
    "\n",
    "* **Subscription length and total usage have a very low negative correlation of around -0.0022.**\n",
    "\n",
    "\n",
    "* **This suggests that changes in subscription length are not strongly associated with changes in total data usage.**\n",
    "\n",
    "\n",
    "### `Subscription Length and Churn`\n",
    "\n",
    "* **Subscription length and churn have a very low positive correlation of about 0.0023.** \n",
    "\n",
    "\n",
    "* **There is almost no linear relationship between subscription length and churn behavior means length of a subscription does not have a strong impact on whether a customer churns.**\n",
    "\n",
    "\n",
    "### `Monthly Bill and Total Usage`\n",
    "\n",
    "* **Monthly bill amount and total usage have an extremely low positive correlation of about 0.0032.**\n",
    "\n",
    "\n",
    "* **This indicates that there is almost no linear relationship between a customer's monthly bill and their total data usage. It means the amount of data a customer uses does not have a strong impact on their monthly bill.**\n",
    "\n",
    "### `Monthly Bill and Churn`\n",
    "\n",
    "* **Monthly bill amount and churn have an extremely low negative correlation of approximately -0.0002.**\n",
    "\n",
    "\n",
    "* **This suggests that there is hardly any linear relationship between monthly bill amount and churn behavior.** \n",
    "\n",
    "\n",
    "* **The amount a customer is billed monthly does not have a strong impact on whether they churn.**\n",
    "\n",
    "### `Total Usage and Churn`\n",
    "\n",
    "* **Total usage and churn have a very low negative correlation of about -0.0028.**\n",
    "\n",
    "\n",
    "* **There is almost no linear relationship between a customer's data usage and their likelihood to churn.**\n",
    "\n",
    "\n",
    "* **The amount of data a customer uses does not have a strong impact on whether they churn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1f3054",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomerID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['CustomerID', 'Name'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82275164",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a955b",
   "metadata": {},
   "source": [
    "**Removed ['CustomerID' & 'Name'] variables becuase they are irrelevant for analysis. Now dataset left with 6 variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c306b752",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check for typo's & suspicious values and rechecking dataset \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m      4\u001b[0m     unique_values \u001b[38;5;241m=\u001b[39m df[column]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnique values in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Check for typo's & suspicious values and rechecking dataset \n",
    "\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Unique values in '{column}': {unique_values}\")\n",
    "    print()\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "data_types = df.dtypes\n",
    "\n",
    "print('-'*50)\n",
    "print(\"Missing values:\")\n",
    "print(missing_values)\n",
    "print()\n",
    "\n",
    "x = df.duplicated().sum()\n",
    "print('-'*50)\n",
    "print(\"Duplicate values:\", x)\n",
    "print()\n",
    "\n",
    "\n",
    "y = df.shape\n",
    "print('-'*50)\n",
    "print(\"Shape of Dataset:\", y)\n",
    "print()\n",
    "\n",
    "z = df.columns\n",
    "print('-'*50)\n",
    "print(\"Columns of Datset:\", z)\n",
    "print()\n",
    "\n",
    "print('-'*50)\n",
    "print(\"\\nData types:\")\n",
    "print(data_types)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb7859c",
   "metadata": {},
   "source": [
    "**The dataset has been cleaned and no suspicious values or typos have been found.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a9a41",
   "metadata": {},
   "source": [
    "# `Outliers Treatment`\n",
    "\n",
    "It is important to find and remove unusual or extreme data points, also known as outliers. Outliers can affect the model's understanding of the data and lead to wrong predictions. By identifying and removing outliers, the model becomes more accurate and reliable, and can better capture the actual patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f660d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Adjust figure size based on the number of columns\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m num_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m      3\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_columns \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m16\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Adjust figure size based on the number of columns\n",
    "num_columns = len(df.columns)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "if num_columns > 16:\n",
    "    fig.set_size_inches(10, num_columns * 0.6)\n",
    "else:\n",
    "    fig.set_size_inches(num_columns, 6)\n",
    "\n",
    "# Create a boxplot for each column\n",
    "df.boxplot(ax=ax)\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title(\"Boxplot of Columns\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Values\")\n",
    "\n",
    "# Rotate the x-axis labels for better readability (optional)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3de4af",
   "metadata": {},
   "source": [
    "**There is no outlier present in any of the variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6193a6bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make a copy of cleaned data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Make a copy of cleaned data\n",
    "\n",
    "df_cleaned = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09da425",
   "metadata": {},
   "source": [
    "# `Feature Encoding`\n",
    "\n",
    "Feature encoding techniques converts categorical data (like labels) into numbers that algorithms understand. This is essential because algorithms work with numbers, ensuring valuable info from categories isn't lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "868b4e5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m categorical_columns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m      2\u001b[0m categorical_columns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a4328",
   "metadata": {},
   "source": [
    "**There are two variables, which needs to be encode.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7d7273",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(df, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m], drop_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.get_dummies(df, columns=['Gender', 'Location'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2bdab30",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3536f146",
   "metadata": {},
   "source": [
    "**Here I apply One Hot Encoding becuase both variables have less number of categories.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e7ce710",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d0106",
   "metadata": {},
   "source": [
    "**Data types of all variables is fine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3847f",
   "metadata": {},
   "source": [
    "# `Checking Distribution of data`\n",
    "\n",
    "* In machine learning, it is important to check how the data is spread across different values. This is called the distribution of data.\n",
    "\n",
    "\n",
    "* If the data is skewed, meaning that there are more data points in one area than others, it can affect the performance of our models. For example, if the data is skewed towards the majority class, the model might perform well on the majority class but poorly on the minority class.\n",
    "\n",
    "\n",
    "* By understanding the data distribution, we can choose appropriate strategies to handle imbalances, outliers, or other issues. This will ensure that our model works well across all scenarios and accurately represents the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "252af253",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc90af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins according to Sturges' Rule: 17\n"
     ]
    }
   ],
   "source": [
    "# Select appropriate bin size using sturges_rule\n",
    "\n",
    "import math\n",
    "\n",
    "def sturges_rule(num_data_points):\n",
    "    k = 1 + math.log2(num_data_points)\n",
    "    return int(k)\n",
    "\n",
    "# Example usage\n",
    "num_data_points = 100000\n",
    "bins = sturges_rule(num_data_points)\n",
    "print(\"Number of bins according to Sturges' Rule:\", bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b92b6a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check distribution of all continuous variables\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m continuous_vars \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Exclude binary variables from the list\u001b[39;00m\n\u001b[1;32m      5\u001b[0m binary_vars \u001b[38;5;241m=\u001b[39m [var \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m continuous_vars \u001b[38;5;28;01mif\u001b[39;00m df[var]\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# check distribution of all continuous variables\n",
    "continuous_vars = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Exclude binary variables from the list\n",
    "binary_vars = [var for var in continuous_vars if df[var].nunique() == 2]\n",
    "\n",
    "# Exclude binary variables from the continuous variables\n",
    "continuous_vars = [var for var in continuous_vars if var not in binary_vars]\n",
    "\n",
    "# Plot the distribution of each continuous variable\n",
    "for var in continuous_vars:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(data=df, x=var, bins=17, kde=True)\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of {var}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bd2974f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check skewness of all continuous variables\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df[continuous_vars]\u001b[38;5;241m.\u001b[39mskew()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# check skewness of all continuous variables\n",
    "df[continuous_vars].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b766e7a",
   "metadata": {},
   "source": [
    "**All variables are approx normally distributed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2fd60",
   "metadata": {},
   "source": [
    "# `Check colinearity between variables`\n",
    "\n",
    "* In machine learning, it is important to check if there are variables that are highly correlated with each other. If there are, then we should remove one of them to improve the stability and interpretability of the model.\n",
    "\n",
    "\n",
    "* Highly correlated variables are redundant, meaning that they provide the same information. This can cause the model to give too much importance to one variable, leading to overfitting. Overfitting is when the model learns the training data too well and does not generalize well to new data.\n",
    "\n",
    "\n",
    "* By removing highly correlated variables, we can make the model focus on distinct and meaningful information. This makes the model more reliable and understandable. It also helps to prevent multicollinearity, which is a problem that can distort the model's predictions and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b36ffdb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#checking colinearity between variables\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m8\u001b[39m)) \u001b[38;5;66;03m#(width,height)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(corr_matrix, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m, square\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#checking colinearity between variables\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(14,8)) #(width,height)\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5a9f9",
   "metadata": {},
   "source": [
    "**No pair of variable is highly corrleated.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305d7b8",
   "metadata": {},
   "source": [
    "# `Diving data into train and test set` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03a06ca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChurn\u001b[39m\u001b[38;5;124m\"\u001b[39m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChurn\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "x = df.drop(\"Churn\",axis=1)\n",
    "y = df['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b609f9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 3\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(x, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f0cc2",
   "metadata": {},
   "source": [
    "**Take 70% data for training and 30% for testing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac957d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb2d34",
   "metadata": {},
   "source": [
    "# `Feature Scaling` \n",
    "\n",
    "* In machine learning, it is important to make sure that all features are on the same scale. This is called feature scaling.\n",
    "\n",
    "\n",
    "* Feature scaling is important because it ensures that no single feature dominates the learning process. This is because many machine learning algorithms use distance-based calculations, and if features are on different scales, those with larger values could disproportionately influence the results.\n",
    "\n",
    "\n",
    "* Feature scaling also helps to improve the convergence of gradient-based optimization algorithms, which can lead to faster training.\n",
    "\n",
    "\n",
    "* By scaling features, we create a balanced playing field for each feature. This allows the algorithm to make fair and accurate comparisons, which can lead to better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244aeef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = df.columns\n",
    "columns_needs_to_be_scaled = []\n",
    "\n",
    "for column in columns_to_check:\n",
    "    if (df[column] > 1).any() or (df[column] < 0).any():\n",
    "        columns_needs_to_be_scaled.append(column)\n",
    "\n",
    "print(\"Columns with values greater than 1 or less than 0:\")\n",
    "print(columns_needs_to_be_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05c1f9",
   "metadata": {},
   "source": [
    "**These four columns need scaling, ['Age', 'Subscription_Length_Months', 'Monthly_Bill', 'Total_Usage_GB']. I decide to use MinMax Scaler becuase majority of variables in binary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "columns_to_scale = ['Age', 'Subscription_Length_Months', 'Monthly_Bill', 'Total_Usage_GB']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
    "\n",
    "X_test[columns_to_scale] = scaler.transform(X_test[columns_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb5d53",
   "metadata": {},
   "source": [
    "* We use the .fit_transform() method on training data to calculate the scaling parameters (such as mean and standard deviation) and then apply the transformation. This ensures that the training data is scaled properly.\n",
    "\n",
    "\n",
    "* However, we use the .transform() method on test data because we do not want to change the scaling parameters based on the test data. Instead, we want to apply the same scaling that was learned from the training data. This maintains the consistency of scaling between training and testing data, which is more realistic and prevents the model from learning information from the test data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef1ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e234ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b47457",
   "metadata": {},
   "source": [
    "# `Check for the class imbalance`\n",
    "\n",
    "* In machine learning, it is important to check if the data is balanced or not. This is called class imbalance.\n",
    "\n",
    "\n",
    "* Class imbalance happens when there are more data points in one class than in the other class. This can affect the performance of the model. For example, if the majority class has more data points than the minority class, the model might perform well on the majority class but poorly on the minority class.\n",
    "\n",
    "\n",
    "* By identifying class imbalance, we can employ strategies to address the issue. These strategies include resampling, adjusting class weights, or using different evaluation metrics. This will help us create a fair and accurate model that performs well across all classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a543d50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChurn\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m      2\u001b[0m class_counts\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "class_counts = df['Churn'].value_counts()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9f5d311",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m)) \n\u001b[0;32m----> 2\u001b[0m class_counts\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass Distribution\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_counts' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6)) \n",
    "class_counts.plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12836a5",
   "metadata": {},
   "source": [
    "**The churn variables are almost evenly distributed, which means that there is no problem of class imbalance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a216c9b",
   "metadata": {},
   "source": [
    "# `Feature Selection using Random Forest Feature Importance Method`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "119b2b21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      3\u001b[0m rf_classifier \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m rf_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      7\u001b[0m importances \u001b[38;5;241m=\u001b[39m rf_classifier\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[1;32m      9\u001b[0m feature_importance_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m: X_train\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m'\u001b[39m: importances})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "importances = rf_classifier.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2cabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check optimal number of features\n",
    "\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "cumulative_importance = np.cumsum(importances[sorted_indices])\n",
    "\n",
    "plt.plot(range(1, len(importances) + 1), cumulative_importance, 'b-')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cumulative Importance')\n",
    "plt.title('Cumulative Importance of Features')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d712fa",
   "metadata": {},
   "source": [
    "**The graph shows that only 4 variables are important for the analysis. These variables are:**\n",
    "\n",
    "* **Monthly bill**\n",
    "\n",
    "\n",
    "* **Total usage in GB**\n",
    "\n",
    "\n",
    "* **Age**\n",
    "\n",
    "\n",
    "* **Subscription length in months**\n",
    "\n",
    "\n",
    "* **These 4 variables together account for approximately 94% of the importance for the analysis. This means that they are the most important variables to consider when trying to predict customer churn.**\n",
    "\n",
    "\n",
    "* **Here, I will use all the variables for analysis because there are not many variables. If the model does not perform well or overfits, I will remove variables and build a model on the features that contribute more to the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c055c6",
   "metadata": {},
   "source": [
    "# `Check multi-colinearity between variables`\n",
    "\n",
    "* In machine learning, it is important to check if there are variables that are highly correlated with each other. This is called multicollinearity.\n",
    "\n",
    "\n",
    "* Multicollinearity can cause problems with the model, such as making it unstable and difficult to interpret. This is because correlated variables are redundant, meaning that they provide the same information. This can cause the model to give too much importance to one variable, leading to overfitting.\n",
    "\n",
    "\n",
    "* By identifying and addressing multicollinearity, we can make the model more reliable and interpretable. This improves the model's predictive power and makes it easier to understand how the variables affect the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40457ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking multicolinearity of X_train_resampled\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Variable\"] = X_train.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "vif = vif.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6c0fa",
   "metadata": {},
   "source": [
    "**The training data does not contain multicollinearity because all of the variables have a VIF value less than 5.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a540b882",
   "metadata": {},
   "source": [
    "# `Model Building: Machine Learning Algorithms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1185dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing algorithms, metrics and time\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81daac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\n",
    "    LogisticRegression(n_jobs=-1, random_state=42),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    KNeighborsClassifier(n_jobs=-1),\n",
    "    GaussianNB(),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "    XGBClassifier(n_jobs=-1, random_state=42),\n",
    "    SVC(random_state=42)\n",
    "]\n",
    "\n",
    "# Initialize the results dictionary for training data\n",
    "results_train = {\n",
    "    'Algorithm': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1-score': [],\n",
    "    'Building Time (s)': []\n",
    "}\n",
    "\n",
    "# Apply the algorithms and calculate performance metrics for training data\n",
    "for algorithm in algorithms:\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    algorithm_name = type(algorithm).__name__\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    y_train_pred = algorithm.predict(X_train)\n",
    "\n",
    "    accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "    end_time = time.time()  # End timer\n",
    "    building_time = end_time - start_time\n",
    "\n",
    "    results_train['Algorithm'].append(algorithm_name)\n",
    "    results_train['Accuracy'].append(accuracy)\n",
    "    results_train['Precision'].append(precision)\n",
    "    results_train['Recall'].append(recall)\n",
    "    results_train['F1-score'].append(f1)\n",
    "    results_train['Building Time (s)'].append(building_time)\n",
    "\n",
    "# Create a dataframe for the training data results\n",
    "df_train = pd.DataFrame(results_train)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c4b923",
   "metadata": {},
   "source": [
    "* **Decision Tree, Random Forest, and Gradient Boosting achieved perfect accuracy on the training data, suggesting potential overfitting.**\n",
    "\n",
    "\n",
    "* **Logistic Regression, GaussianNB, and AdaBoost have relatively low accuracy and F1-score.**\n",
    "\n",
    "\n",
    "* **KNeighborsClassifier and XGBClassifier have moderate accuracy and F1-score.**\n",
    "\n",
    "\n",
    "* **SVC has moderate accuracy but high building time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = {\n",
    "    'Algorithm': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1-score': []\n",
    "}\n",
    "\n",
    "# Apply the algorithms and calculate performance metrics for test data\n",
    "for algorithm in algorithms:\n",
    "    algorithm_name = type(algorithm).__name__\n",
    "    y_test_pred = algorithm.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    results_test['Algorithm'].append(algorithm_name)\n",
    "    results_test['Accuracy'].append(accuracy)\n",
    "    results_test['Precision'].append(precision)\n",
    "    results_test['Recall'].append(recall)\n",
    "    results_test['F1-score'].append(f1)\n",
    "\n",
    "# Create a dataframe for the test data results\n",
    "df_test = pd.DataFrame(results_test)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff4f2e",
   "metadata": {},
   "source": [
    "* **Most algorithms have mediocre performance on the test data, with accuracy close to random guessing (around 0.5).**\n",
    "\n",
    "\n",
    "* **Decision Tree, Random Forest, and Gradient Boosting also show subpar results, indicating overfitting.**\n",
    "\n",
    "\n",
    "* **No algorithm is performing well, so I decided to take only 4 variables that contribute more to the target variable and build the model again.**\n",
    "\n",
    "\n",
    "* **I removed the age and location variables because they are not good predictors of whether a customer will churn or not.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb15cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[['Monthly_Bill', 'Total_Usage_GB', 'Age', 'Subscription_Length_Months']]\n",
    "\n",
    "X_test = X_test[['Monthly_Bill', 'Total_Usage_GB', 'Age', 'Subscription_Length_Months']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fda754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train columns:', X_train.columns)\n",
    "print('-'*120)\n",
    "print('X_test columns:', X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ef4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\n",
    "    LogisticRegression(n_jobs=-1, random_state=42),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    KNeighborsClassifier(n_jobs=-1),\n",
    "    GaussianNB(),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "    XGBClassifier(n_jobs=-1, random_state=42),\n",
    "    SVC(random_state=42)\n",
    "]\n",
    "\n",
    "# Initialize the results dictionary for training data\n",
    "results_train = {\n",
    "    'Algorithm': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1-score': [],\n",
    "    'Building Time (s)': []\n",
    "}\n",
    "\n",
    "# Apply the algorithms and calculate performance metrics for training data\n",
    "for algorithm in algorithms:\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    algorithm_name = type(algorithm).__name__\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    y_train_pred = algorithm.predict(X_train)\n",
    "\n",
    "    accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "    end_time = time.time()  # End timer\n",
    "    building_time = end_time - start_time\n",
    "\n",
    "    results_train['Algorithm'].append(algorithm_name)\n",
    "    results_train['Accuracy'].append(accuracy)\n",
    "    results_train['Precision'].append(precision)\n",
    "    results_train['Recall'].append(recall)\n",
    "    results_train['F1-score'].append(f1)\n",
    "    results_train['Building Time (s)'].append(building_time)\n",
    "\n",
    "# Create a dataframe for the training data results\n",
    "df_train = pd.DataFrame(results_train)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b51c921",
   "metadata": {},
   "source": [
    "* **Decision Tree and Random Forest still achieve perfect accuracy, likely overfitting again.**\n",
    "\n",
    "\n",
    "* **Logistic Regression, GaussianNB, and AdaBoost have improved slightly.**\n",
    "\n",
    "\n",
    "* **KNeighborsClassifier and XGBClassifier remain moderate.**\n",
    "\n",
    "\n",
    "* **SVC still has high building time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = {\n",
    "    'Algorithm': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1-score': []\n",
    "}\n",
    "\n",
    "# Apply the algorithms and calculate performance metrics for test data\n",
    "for algorithm in algorithms:\n",
    "    algorithm_name = type(algorithm).__name__\n",
    "    y_test_pred = algorithm.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    results_test['Algorithm'].append(algorithm_name)\n",
    "    results_test['Accuracy'].append(accuracy)\n",
    "    results_test['Precision'].append(precision)\n",
    "    results_test['Recall'].append(recall)\n",
    "    results_test['F1-score'].append(f1)\n",
    "\n",
    "# Create a dataframe for the test data results\n",
    "df_test = pd.DataFrame(results_test)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dcb884",
   "metadata": {},
   "source": [
    "* **Decision Tree and Random Forest overfits.**\n",
    "\n",
    "\n",
    "* **No algorithm is performing well, so I decided to build model using neural networks.**\n",
    "\n",
    "\n",
    "* **If the neural network does not provide better results than machine learning algorithms, then I will build ensembles of random forests because random forests achieve perfect scores in all metrics in the training data. Decision trees also achieve perfect scores in all metrics in the training data, but I have already tried and tested multiple ensembles that use decision trees. Therefore, I will now build ensembles using random forests**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd508fc",
   "metadata": {},
   "source": [
    "# `Model Building: Neural Network`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abacd02c",
   "metadata": {},
   "source": [
    "## `Defining Callbacks for Early Stopping and Model Checkpoint`\n",
    "\n",
    "* Best model will save to pwd (present working directory) as 'Churn Classifier.h5'\n",
    "\n",
    "\n",
    "* Early stopping is a regularization technique that stops the training of a neural network before it reaches the maximum number of epochs or iterations. It is important because it helps to prevent overfitting.\n",
    "\n",
    "\n",
    "* Early stopping is a technique that stops the training of a model when the validation loss stops improving. If you set restore_best_weights to False, the model will be saved at the end of the training, even if the validation loss has not improved. This could result in the model being saved at a point where it is overfitting the training data. By setting restore_best_weights to True, the model will be saved at the epoch with the best validation loss. This ensures that the model that is saved is the one that is most likely to generalize well to new data.\n",
    "\n",
    "\n",
    "* When you set restore_best_weights=True in the EarlyStopping callback, it ensures that the model's weights are restored to the state they were in at the epoch with the lowest validation loss. This means that when you evaluate the model after training, the evaluation metrics, including accuracy and other metrics, are computed based on the weights of the model that performed the best on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d997187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EarlyStopping and ModelCheckpoint callbacks\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        # The metric to monitor. This is usually the validation loss.\n",
    "    min_delta=0.001,           # The minimum amount of improvement required to consider the model to have improved.\n",
    "    patience=10,               # The number of epochs to wait without improvement before stopping the training.\n",
    "    verbose=1,                 # Print output\n",
    "    mode='auto',               # Consider mode of the metric as min or max.\n",
    "    restore_best_weights=True  # Whether to restore the model weights at the epoch with the best validation loss.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0fb849",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('ChurnClassifier.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0c8bf",
   "metadata": {},
   "source": [
    "## `Build Neural Network Architectures & Fit Model on Training Data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10399651",
   "metadata": {},
   "source": [
    "### `Architecture I`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7bb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model]\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a16a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_split=0.3, callbacks=[early_stopping, checkpoint],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e1ecb",
   "metadata": {},
   "source": [
    "### `Architecture II`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb03b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# layers\n",
    "model.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = 4))\n",
    "model.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compile the model]\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efcf0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_split=0.3, callbacks=[early_stopping, checkpoint],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f8ba9",
   "metadata": {},
   "source": [
    "### `Architecture III`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083bca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the NN\n",
    "model = Sequential()\n",
    "\n",
    "# layers\n",
    "model.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = 4))\n",
    "model.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Compiling the ANN\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_split=0.3, callbacks=[early_stopping, checkpoint],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1267af1",
   "metadata": {},
   "source": [
    "### `Architecture IV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e160692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input layer with BatchNormalization and Activation (ReLU)\n",
    "model.add(Dense(10, input_dim=4, kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# First hidden layer with BatchNormalization, Activation (ReLU), and Dropout\n",
    "model.add(Dense(10, kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))  # 20% dropout\n",
    "\n",
    "# Second hidden layer with BatchNormalization, Activation (ReLU), and Dropout\n",
    "model.add(Dense(5, kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))  # 10% dropout\n",
    "\n",
    "# Output layer with Sigmoid activation\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff07c03",
   "metadata": {},
   "source": [
    "**`Batch Normalization`** is used before the activation function in neural networks to enhance training stability and speed up convergence. By normalizing inputs within each batch, it maintains stable distribution and prevents extreme activations, leading to faster learning and better performance.\n",
    "\n",
    "\n",
    "**`\"He_normal\"`** is a weight initialization technique in neural networks. It sets initial weights to encourage effective learning, especially with ReLU activation. It helps prevent vanishing gradient and works well in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb5b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_split=0.3, callbacks=[early_stopping, checkpoint],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c0da8",
   "metadata": {},
   "source": [
    "### `Architecture V`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network\n",
    "model = Sequential([\n",
    "    Dense(64, activation = 'relu', input_dim = 4),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f20a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[early_stopping, checkpoint],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a931ea6",
   "metadata": {},
   "source": [
    "* **I have tried multiple ANN architectures, but none of them have provided good results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a95d2",
   "metadata": {},
   "source": [
    "## `Ensembles of Random Forest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7fb4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize base estimator (Random Forest)\n",
    "base_estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "adaboost_model = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)\n",
    "gradient_boost_model = GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
    "xgboost_model = XGBClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "models = ['AdaBoost', 'Gradient Boosting', 'XGBoost']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Building Time']\n",
    "results_train = {metric: [] for metric in metrics}\n",
    "results_test = {metric: [] for metric in metrics}\n",
    "\n",
    "# Train and evaluate models\n",
    "for model in [adaboost_model, gradient_boost_model, xgboost_model]:\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Predict on the training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # Calculate metrics on training data\n",
    "    accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "    precision_train = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    recall_train = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    f1_train = f1_score(y_train, y_train_pred, average='weighted')\n",
    "    building_time = end_time - start_time\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics on test data\n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "    precision_test = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    recall_test = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    # Append metrics to the results dictionaries\n",
    "    results_train['Accuracy'].append(accuracy_train)\n",
    "    results_train['Precision'].append(precision_train)\n",
    "    results_train['Recall'].append(recall_train)\n",
    "    results_train['F1 Score'].append(f1_train)\n",
    "    results_train['Building Time'].append(building_time)\n",
    "\n",
    "    results_test['Accuracy'].append(accuracy_test)\n",
    "    results_test['Precision'].append(precision_test)\n",
    "    results_test['Recall'].append(recall_test)\n",
    "    results_test['F1 Score'].append(f1_test)\n",
    "    results_test['Building Time'].append(building_time)\n",
    "\n",
    "# Create DataFrames from the results\n",
    "results_train_df = pd.DataFrame(results_train, index=models)\n",
    "results_test_df = pd.DataFrame(results_test, index=models)\n",
    "\n",
    "# Display the DataFrames\n",
    "print(\"Training Data Results:\")\n",
    "results_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest Data Results:\")\n",
    "results_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667932e4",
   "metadata": {},
   "source": [
    "* **The performance of ensembles can vary, but they are usually not much better than individual algorithms.**\n",
    "\n",
    "\n",
    "* **None of the ensembles provided the desired results. So, I decided to trial and error using a different approach.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d764cee",
   "metadata": {},
   "source": [
    "# `Model Building: PCA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94366989",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.get_dummies(df_cleaned, columns=['Gender', 'Location'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7040da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_cleaned.drop('Churn', axis=1)\n",
    "y = df_cleaned['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08cf20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1369be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d2a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scaled numpy arrays back to DataFrames\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16766486",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4452a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9075d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ebff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11681537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Plot the scree plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3cfb5a",
   "metadata": {},
   "source": [
    "**8 out of 9 principal components (PCs) capture approximately 98% of the variance of the data. Therefore, I will select 8 PCs for further analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad67fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=8)  # Select 8 principal components\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bae1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca.shape, X_test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cbbe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\n",
    "    LogisticRegression(n_jobs=-1, random_state=42),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    KNeighborsClassifier(n_jobs=-1),\n",
    "    GaussianNB(),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "    XGBClassifier(n_jobs=-1, random_state=42),\n",
    "    SVC(random_state=42)\n",
    "]\n",
    "\n",
    "# Initialize the results dictionary for training data\n",
    "results_train = {\n",
    "    'Algorithm': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1-score': [],\n",
    "    'Building Time (s)': []\n",
    "}\n",
    "\n",
    "# Apply the algorithms and calculate performance metrics for training data using X_train_pca\n",
    "for algorithm in algorithms:\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    algorithm_name = type(algorithm).__name__\n",
    "    algorithm.fit(X_train_pca, y_train)\n",
    "    y_train_pred = algorithm.predict(X_train_pca)\n",
    "\n",
    "    accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "    end_time = time.time()  # End timer\n",
    "    building_time = end_time - start_time\n",
    "\n",
    "    results_train['Algorithm'].append(algorithm_name)\n",
    "    results_train['Accuracy'].append(accuracy)\n",
    "    results_train['Precision'].append(precision)\n",
    "    results_train['Recall'].append(recall)\n",
    "    results_train['F1-score'].append(f1)\n",
    "    results_train['Building Time (s)'].append(building_time)\n",
    "\n",
    "# Create a DataFrame for the training data results\n",
    "df_train = pd.DataFrame(results_train)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651353c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = {\n",
    "    'Algorithm': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1-score': []\n",
    "}\n",
    "\n",
    "# Apply the algorithms and calculate performance metrics for test data using X_test_pca\n",
    "for algorithm in algorithms:\n",
    "    algorithm_name = type(algorithm).__name__\n",
    "    y_test_pred = algorithm.predict(X_test_pca)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    results_test['Algorithm'].append(algorithm_name)\n",
    "    results_test['Accuracy'].append(accuracy)\n",
    "    results_test['Precision'].append(precision)\n",
    "    results_test['Recall'].append(recall)\n",
    "    results_test['F1-score'].append(f1)\n",
    "\n",
    "# Create a DataFrame for the test data results\n",
    "df_test = pd.DataFrame(results_test)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa5ff2",
   "metadata": {},
   "source": [
    "**There's no significant improvement in performance with PCA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e0238",
   "metadata": {},
   "source": [
    "**`Final Model:` XGBClassifier is the best-performing algorithm on both the training and test data, using 9 variables, 4 variables, or PCA. It has the highest accuracy, precision, recall, and F1-score on all three. Therefore, I will build the final model using XGBoost Classifier using 9 variables.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551aff0a",
   "metadata": {},
   "source": [
    "# `Final Model: XGBoost Using 9 variables`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017bf4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_cleaned.drop(\"Churn\",axis=1)\n",
    "y = df_cleaned['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad161be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeffc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = ['Age', 'Subscription_Length_Months', 'Monthly_Bill', 'Total_Usage_GB']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
    "\n",
    "X_test[columns_to_scale] = scaler.transform(X_test[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ce919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost classifier\n",
    "xgb_classifier = XGBClassifier(\n",
    "    objective='binary:logistic',  # For binary classification\n",
    "    random_state=42               # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train (fit) the model\n",
    "xgb_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = xgb_classifier.predict(X_train)\n",
    "y_test_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate metrics for train set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Train': [train_accuracy, train_precision, train_recall, train_f1],\n",
    "    'Test': [test_accuracy, test_precision, test_recall, test_f1]\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b997f",
   "metadata": {},
   "source": [
    "# `Hyperparameter Tunning`\n",
    "\n",
    "Hyperparameters are settings in machine learning algorithms that control how the algorithm works. They are like the knobs and dials on a machine, and they can be adjusted to improve the performance of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a8187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [50, 100, 150]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GridSearchCV with recall as the scoring metric\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_classifier,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV on the training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77426fe",
   "metadata": {},
   "source": [
    "**According to the problem statement, false negatives are more important to reduce. Therefore, I selected recall as a metric to focus on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc2604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from the search\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_test_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Calculate recall for the test set\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Test Recall with Best Model:\", test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae3723",
   "metadata": {},
   "source": [
    "**After hyperparameter tuning, the performance of the model did not improve. Hence, I choose XGBoost CLassifier without hyperparameter tunning and perform cross-validation on it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e29ae7",
   "metadata": {},
   "source": [
    "# `Cross Validation`\n",
    "\n",
    "* Cross-validation is a technique used to evaluate the performance of a machine learning model. It works by dividing the data into several parts, called folds. The model is then trained on a subset of the data and tested on the remaining folds. This process is repeated several times, and the results are averaged to get an estimate of the model's performance.\n",
    "\n",
    "\n",
    "* Cross-validation is necessary because it helps us avoid overfitting the model to the training data.\n",
    "\n",
    "\n",
    "* Cross-validation is a way to make sure that our model is not just memorizing the training data, but that it is actually learning to make accurate predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation of accuracy\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(xgb_classifier, X_train, y_train, cv=5, scoring='accuracy',n_jobs=-1)\n",
    "\n",
    "print(\"Cross-Validation Scores (Accuracy):\", scores)\n",
    "print()\n",
    "print(\"Mean Accuracy Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation of recall\n",
    "\n",
    "scores = cross_val_score(xgb_classifier, X_train, y_train, cv=5, scoring='recall',n_jobs=-1)\n",
    "\n",
    "print(\"Cross-Validation Scores (Recall):\", scores)\n",
    "print()\n",
    "print(\"Mean Recall Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54677bf9",
   "metadata": {},
   "source": [
    "# `Finding Optimal Threshold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe of probabilites, actual labels, predicted labels for genral check\n",
    "\n",
    "probabilities = xgb_classifier.predict_proba(X_train)\n",
    "\n",
    "predicted_labels = xgb_classifier.predict(X_train)\n",
    "\n",
    "results = pd.DataFrame({'Probability': probabilities[:, 1],\n",
    "                        'Actual_Label': y_train,\n",
    "                        'Predicted_Label': predicted_labels})\n",
    "\n",
    "results.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d000988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check evaulation metrics for different thresholds\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Creating an empty DataFrame to store the evaluation metrics\n",
    "metrics_df = pd.DataFrame(columns=['Probability', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Iterating over the probability thresholds\n",
    "for threshold in thresholds:\n",
    "    # Converting probabilities to predicted labels based on the threshold\n",
    "    predicted_labels = (probabilities[:, 1] >= threshold).astype(int)\n",
    "\n",
    "    # Calculating evaluation metrics\n",
    "    accuracy = accuracy_score(y_train, predicted_labels)\n",
    "    precision = precision_score(y_train, predicted_labels, zero_division=1)\n",
    "    recall = recall_score(y_train, predicted_labels, zero_division=1)\n",
    "    f1 = f1_score(y_train, predicted_labels, zero_division=1)\n",
    "\n",
    "    # Adding the metrics to the DataFrame\n",
    "    metrics_df = pd.concat([metrics_df, pd.DataFrame({'Probability': [threshold],\n",
    "                                                      'Accuracy': [accuracy],\n",
    "                                                      'Precision': [precision],\n",
    "                                                      'Recall': [recall],\n",
    "                                                      'F1 Score': [f1]})], ignore_index=True)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5513a7",
   "metadata": {},
   "source": [
    "## `Check the Trade-Off Between Accuracy, Sensitivity, Specitivity, F-1 Score and Probabilities (Thresholds)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate predicted probabilities for the train data\n",
    "train_probabilities = xgb_classifier.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Apply different thresholds to obtain binary predictions for train data\n",
    "thresholds = np.arange(0, 1.1, 0.1)\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "accuracy = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    train_predictions = (train_probabilities >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, train_predictions).ravel()\n",
    "    sensitivity.append(tp / (tp + fn))\n",
    "    specificity.append(tn / (tn + fp))\n",
    "    accuracy.append((tp + tn) / (tp + tn + fp + fn))\n",
    "    f1_scores.append(f1_score(y_train, train_predictions))\n",
    "\n",
    "# Plot the sensitivity, specificity, accuracy, and F1 score against probability thresholds\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(thresholds, sensitivity, label='Sensitivity')\n",
    "plt.plot(thresholds, specificity, label='Specificity')\n",
    "plt.plot(thresholds, accuracy, label='Accuracy')\n",
    "plt.plot(thresholds, f1_scores, label='F1 Score')\n",
    "plt.xlabel('Probability Threshold')\n",
    "plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Metrics vs. Probability Threshold')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33900411",
   "metadata": {},
   "source": [
    "**The graph indicates that 0.5 is the best threshold point, where accuracy, sensitivity, specificity, and F1 score are all stable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81360d",
   "metadata": {},
   "source": [
    "# `Model Evaulation (XG Boost)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2cb8fe",
   "metadata": {},
   "source": [
    "## `Train & Test Data Metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for train data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
    "train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
    "train_f1_score = f1_score(y_train, y_train_pred, average='macro')\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics for test data\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "test_f1_score = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "# Create a DataFrame to store the evaluation metrics\n",
    "metrics_data = {\n",
    "    'Dataset': ['Train', 'Test'],\n",
    "    'Accuracy': [train_accuracy, test_accuracy],\n",
    "    'Precision': [train_precision, test_precision],\n",
    "    'Recall': [train_recall, test_recall],\n",
    "    'F1-score': [train_f1_score, test_f1_score]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed2be82",
   "metadata": {},
   "source": [
    "## `Confusion Matrix`\n",
    "\n",
    "Confusion matrix is a table that is used to evaluate the performance of a machine learning model. It shows how many instances were correctly classified as positive (true positives), incorrectly classified as positive (false positives), correctly classified as negative (true negatives), and incorrectly classified as negative (false negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d130517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate predictions for the training set using a threshold of 0.50\n",
    "train_predictions = (xgb_classifier.predict_proba(X_train)[:, 1] >= 0.50).astype(int)\n",
    "train_confusion_matrix = confusion_matrix(y_train, train_predictions)\n",
    "\n",
    "# Calculate predictions for the test set using a threshold of 0.50\n",
    "test_predictions = (xgb_classifier.predict_proba(X_test)[:, 1] >= 0.50).astype(int)\n",
    "test_confusion_matrix = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "# Calculate the total number of samples in each set\n",
    "train_total = len(y_train)\n",
    "test_total = len(y_test)\n",
    "\n",
    "# Calculate the confusion matrix in percentage form for the training set\n",
    "train_confusion_matrix_percent = train_confusion_matrix / train_total * 100\n",
    "\n",
    "# Calculate the confusion matrix in percentage form for the test set\n",
    "test_confusion_matrix_percent = test_confusion_matrix / test_total * 100\n",
    "\n",
    "# Create a dataframe for the confusion matrix\n",
    "confusion_matrix_df = pd.DataFrame({'Training Set': train_confusion_matrix_percent.flatten(),\n",
    "                                    'Test Set': test_confusion_matrix_percent.flatten()},\n",
    "                                   index=['True Positive (%)', 'True Negative (%)', 'False Positive (%)', 'False Negative (%)', ])\n",
    "#\n",
    "confusion_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b520c0",
   "metadata": {},
   "source": [
    "## `ROC-AUC Curve`\n",
    "\n",
    "ROC-AUC curve, or Receiver Operating Characteristic-Area Under Curve, is a graph used in machine learning to measure the performance of a binary classifier. It plots the true positive rate (TPR) against the false positive rate (FPR) at different classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# Calculate predicted probabilities for the train data\n",
    "train_probabilities = xgb_classifier.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Apply the threshold of 0.50 to obtain binary predictions for train data\n",
    "train_predictions = (train_probabilities >= 0.50).astype(int)\n",
    "\n",
    "# Calculate the false positive rate, true positive rate, and thresholds for train data\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, train_probabilities)\n",
    "\n",
    "# Calculate the ROC-AUC score for train data\n",
    "roc_auc_train = roc_auc_score(y_train, train_predictions)\n",
    "\n",
    "# Calculate predicted probabilities for the test data\n",
    "test_probabilities = xgb_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply the threshold of 0.50 to obtain binary predictions for test data\n",
    "test_predictions = (test_probabilities >= 0.50).astype(int)\n",
    "\n",
    "# Calculate the false positive rate, true positive rate, and thresholds for test data\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, test_probabilities)\n",
    "\n",
    "# Calculate the ROC-AUC score for test data\n",
    "roc_auc_test = roc_auc_score(y_test, test_predictions)\n",
    "\n",
    "# Plot the ROC curves\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr_train, tpr_train, label='Train ROC curve (area = {:.2f})'.format(roc_auc_train))\n",
    "plt.plot(fpr_test, tpr_test, label='Test ROC curve (area = {:.2f})'.format(roc_auc_test))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b272b92",
   "metadata": {},
   "source": [
    "# `Important Features in Model Building`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91105e19",
   "metadata": {},
   "source": [
    "**The four variables ['Monthly_Bill', 'Total_Usage_GB', 'Age', and 'Subscription_Length_Months'] are the most important variables in the analysis because they contain 94% of the feature importance or information gain in predicting whether a customer will churn or not.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58250a",
   "metadata": {},
   "source": [
    "# `Save model in a pickle file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acf819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "joblib.dump(xgb_classifier, 'customer_churn_classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf8995",
   "metadata": {},
   "source": [
    "# `Outcome`\n",
    "\n",
    "**The outcome of this customer churn prediction project involves developing a machine learning model to predict whether customers are likely to churn or not. This prediction is based on various customer attributes such as age, gender, location, subscription length, monthly bill, and total usage. The model's primary purpose is to assist in identifying customers who are at a higher risk of churning, enabling the business to take proactive measures to retain them. By using the trained model to predict churn, the company can allocate resources more effectively, personalize engagement strategies, and implement targeted retention efforts. Ultimately, the project's success is measured by the model's ability to make predictions, helping the company reduce churn rates, improve customer satisfaction, and optimize its customer retention strategies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585340d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
